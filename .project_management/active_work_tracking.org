#+TITLE: Active Work Tracking
#+DATE: 2025-01-22
#+AUTHOR: Quantum Ridge
#+FILETAGS: :tracking:active:work:

* Current Project Status

** ACTIVE: Selector-Based Batch Site Processing System
- **Model**: Quantum Ridge
- **Status**: IMPLEMENTATION PHASE - Starting development
- **Priority**: HIGH - Complete automated website processing pipeline

*** Project Objective
Implement comprehensive website crawling and batch JSON generation system using existing selectors as relevance filters. System will:
1. Crawl websites to discover all pages
2. Test stored XPath selectors against each page
3. Filter pages by selector success rate (‚â•90% threshold)
4. Generate AI JSON for qualifying pages
5. Upload batch results to S3 bucket

*** Implementation Phases

**** Phase 1: Core Batch Processor (üîÑ IN PROGRESS)
- **File**: `apps/content_extractor/management/commands/batch_site_processor.py`
- **Components**:
  - Site discovery using enhanced crawler logic
  - Selector testing against discovered pages
  - Relevance filtering by success rate threshold
  - Progress tracking and resumption capability

**** Phase 2: Selector Success Rate Engine (‚è≥ PENDING)
- **Integration**: Enhance existing `extract_content_for_selectors()` method
- **Features**:
  - Calculate percentage of successful selectors per page
  - Handle multiple XPath selectors per field
  - Detailed success/failure reporting
  - Configurable threshold settings

**** Phase 3: S3 Batch Upload System (‚è≥ PENDING)
- **Integration**: Use existing `apps/ai_processing/utils.py` boto3 client
- **Features**:
  - Organized S3 structure: `bucket/domain/date/`
  - Batch manifest generation
  - Upload progress tracking
  - Error handling and retry logic

**** Phase 4: Enhanced Crawler Integration (‚è≥ PENDING)
- **Base**: Extend `scripts/equipment_scrapers/triad_url_scraper.py`
- **Enhancements**:
  - Generalized URL pattern detection
  - Domain-specific crawling rules
  - Depth and page limits
  - URL normalization and deduplication

*** Technical Foundation (‚úÖ AVAILABLE)
- **Database Models**: SiteConfiguration, FieldConfiguration with XPath storage
- **XPath Testing**: `generate_ai_json.py` extraction engine
- **Crawler Base**: `triad_url_scraper.py` with depth-limited crawling
- **AI JSON Generation**: Complete structured output system
- **AWS Integration**: S3 upload capabilities via boto3
- **Interactive Selectors**: Field mapping and configuration system

*** Success Criteria
- [ ] Batch processor command functional
- [ ] Selector success rate calculation accurate
- [ ] Pages filtered by relevance threshold
- [ ] AI JSON generated for qualifying pages only
- [ ] S3 upload organized and reliable
- [ ] System handles 100+ pages automatically
- [ ] Minimal human intervention required

* Implementation Timeline

** Immediate Tasks (Today)
1. **Create batch_site_processor.py command structure**
2. **Implement selector success rate calculation**
3. **Test with airscience.com existing selectors**
4. **Basic progress reporting and logging**

** Next Session Tasks
1. **S3 integration and batch upload**
2. **Enhanced crawler with generalized patterns**
3. **Error handling and resumption logic**
4. **Performance optimization for large sites**

* File Locations

** New Files to Create
- `apps/content_extractor/management/commands/batch_site_processor.py` (main command)
- Enhanced methods in existing files

** Existing Files to Enhance
- `apps/content_extractor/management/commands/generate_ai_json.py` (selector testing)
- `scripts/equipment_scrapers/triad_url_scraper.py` (crawler base)
- `apps/ai_processing/utils.py` (S3 integration)

** Documentation
- Conversation: `.project_management/conversation_logs/quantum-ridge/2025-01-22_session_log.org`
- Architecture updates: `triad_project_architecture.org`

* Testing Strategy

** Test Sites Available
- **airscience.com**: Existing selectors configured via interactive system
- **triadscientific.com**: Crawler already tested on this domain

** Verification Commands
```bash
# Run batch processor with selector filtering
python manage.py batch_site_processor --domain airscience.com --threshold 0.9 --max-pages 20 --dry-run

# Test selector success rate calculation
python manage.py batch_site_processor --domain airscience.com --test-selectors-only

# Full processing with S3 upload
python manage.py batch_site_processor --domain airscience.com --threshold 0.9 --upload-to-s3
```

** Expected Workflow
1. **Discovery**: Find 20-50 pages on airscience.com
2. **Testing**: Apply existing selectors to each page
3. **Filtering**: Keep pages with ‚â•90% selector success
4. **Processing**: Generate AI JSON for 10-25 qualifying pages
5. **Upload**: Batch upload to S3 with organized structure

* Context for Implementation
This system leverages all existing infrastructure (selectors, crawler, JSON generation, S3) into a unified automated pipeline. The selector-based filtering approach eliminates manual page review while ensuring high-quality extraction results.

Key advantage: Uses the interactive selector system as a "proof of relevance" mechanism - if selectors work well on a page, it's relevant for processing. 